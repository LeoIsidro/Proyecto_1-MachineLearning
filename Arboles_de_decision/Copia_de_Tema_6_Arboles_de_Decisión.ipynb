{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # Práctica  5.\n",
        " ----\n",
        "  \n",
        "  Universidad : UTEC \\\\\n",
        "  Curso       : Inteligencia Artificial \\\\\n",
        "  Profesor    : Cristian López Del Alamo \\\\\n",
        "  Tema        : Árboles de Decisión \\\\\n",
        "  \n",
        "\n",
        " ----\n",
        "\n",
        " Nombres y  Apellidos de Integrantes: (No olvide poner el % de participacion)\n",
        " - Integrante 1: Maria Fernanda Surco Vergara (100%)\n",
        " - Integrante 2: Brhandon Gutierrez Soto (100%)\n",
        " - Integrante 3:\n",
        " - Integrante 4:\n",
        "\n",
        "*Una vez concluya la práctica debe subir el link de su colab a este  [Drive]\n",
        "(https://docs.google.com/spreadsheets/d/1XCxGVmf8g29C7RZSPOqxvZHjjRZg45LHWR8mkZOfS1o/edit?usp=drivesdk)*\n"
      ],
      "metadata": {
        "id": "9lQct68MlKo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este ejercicio, usted deberá contruir su propio árbol de desición.\n",
        "- Tenga en cuenta que se trata de un algoritmo recursivo.\n",
        "- El caso base se da cuanto todos los elementos de un nodo tiene las mismas etiquetas, es decir, es un nodo terminal. Luego, el label de ese nodo toma el valor de la etiqueta común.\n",
        "- En el caso que no sea un nodo terminal, el algoritmo debe buscar uno de los\n",
        "feactures por el cual dividirse y para esto use Ganacia de Información (Entropía o Gini).\n",
        "- Divida el dataset usando el feacture que genere una mayor ganancia de información en el padre o un menor GINI y llame recursivamente a la función create_DT.\n",
        "\n",
        "Usted usará la base de datos iris, con 4 características y 3 clases.\n",
        "Tome aleatoriamente 80% de los datos para crear el árbol y el resto para\n",
        "probar el accuracy de la predicción.\n",
        "Finalmente, muestra mediante una matriz de confusión el **accuracy** de su modelo.\n",
        "\n",
        "Trabaje en equipo:\n",
        "\n",
        "[Link de apoyo 1](https://towardsdatascience.com/the-simple-math-behind-3-decision-tree-splitting-criterions-85d4de2a75fe)\n",
        "\n",
        "[Link de apoyo 2](https://www.quantstart.com/articles/Beginners-Guide-to-Decision-Trees-for-Supervised-Machine-Learning/)\n",
        "\n"
      ],
      "metadata": {
        "id": "G19GO8Aflmlv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o74EDY9g86G9",
        "outputId": "24af023e-cd5f-4505-e4b4-d32b05bff92f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sepal_length  sepal_width  petal_length  petal_width species\n",
            "0           5.1          3.5           1.4          0.2  setosa\n",
            "1           4.9          3.0           1.4          0.2  setosa\n",
            "2           4.7          3.2           1.3          0.2  setosa\n",
            "3           4.6          3.1           1.5          0.2  setosa\n",
            "4           5.0          3.6           1.4          0.2  setosa\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "Exactitud: 1.0\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "iris = sns.load_dataset('iris')\n",
        "print(iris.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Nodo:\n",
        " # Defina cuales será sus mimbros datos\n",
        "  def __init__(self, X, Y, index):\n",
        "        self.X = X  # Características\n",
        "        self.Y = Y  # Tipo\n",
        "        self.index = index  # Índice de las muestras en este nodo\n",
        "        self.label = None  # Etiqueta del nodo (se establecerá si es un nodo terminal)\n",
        "        self.feature_index = None  # Índice de la característica de división\n",
        "        self.threshold = None  # Umbral de división\n",
        "        self.child_nodes = {}  # Hijos del nodo\n",
        "\n",
        "  def IsTerminal(self,Y):\n",
        "    # retur true if this node have the sames labeles in Y\n",
        "    return len(np.unique(Y)) == 1\n",
        "\n",
        "  def entropy(self, Y):\n",
        "        _, counts = np.unique(Y, return_counts=True)\n",
        "        probabilities = counts / len(Y)\n",
        "        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))\n",
        "        return entropy\n",
        "\n",
        "  def information_gain(self, Y, left_mask, right_mask):\n",
        "        # Calcula la ganancia de información\n",
        "        parent_entropy = self.entropy(Y)\n",
        "        left_entropy = self.entropy(Y[left_mask])\n",
        "        right_entropy = self.entropy(Y[right_mask])\n",
        "        total_samples = len(Y)\n",
        "\n",
        "        info_gain = parent_entropy - (\n",
        "            (len(Y[left_mask]) / total_samples) * left_entropy +\n",
        "            (len(Y[right_mask]) / total_samples) * right_entropy\n",
        "        )\n",
        "\n",
        "        return info_gain\n",
        "\n",
        "  def BestSplit(self,X,Y):\n",
        "    # write your code here\n",
        "    best_feature_index = None\n",
        "    best_threshold = None\n",
        "    best_info_gain = -1\n",
        "\n",
        "    for x_index in range( X.shape[1]):\n",
        "      val_unique= np.unique(X[:,x_index])\n",
        "      for i in range(len(val_unique)-1):\n",
        "        threshold = (val_unique[i] + val_unique[i + 1]) / 2\n",
        "        left_mask = X[:, x_index] <= threshold\n",
        "        right_mask = ~left_mask\n",
        "        info_gain = self.information_gain(Y, left_mask, right_mask)\n",
        "        if info_gain > best_info_gain:\n",
        "                best_info_gain = info_gain\n",
        "                best_feature_index = x_index\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_feature_index, best_threshold\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DT:\n",
        "    def __init__(self, X, Y, index):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.index = index\n",
        "        self.m_Root = None  # Inicializa m_Root como None\n",
        "        self.create_DT(self.X, self.Y, [index] * len(Y))  # Pasa una lista de índices iguales\n",
        "\n",
        "    def create_DT(self, X, Y, index):\n",
        "      # Verifica si el nodo actual es terminal\n",
        "      if Nodo(X,Y,index).IsTerminal(Y) == 1:\n",
        "          most_common_label = np.unique(Y)[0]  # Get the most common label\n",
        "          self.m_Root = Nodo(X, Y, index)  # Create a terminal node\n",
        "          self.m_Root.label = most_common_label  # Assign the most common label to the terminal node\n",
        "      else:\n",
        "          self.m_Root = Nodo(X, Y, index)\n",
        "          feature, threshold = self.m_Root.BestSplit(X, Y)  # Encuentra la mejor división\n",
        "          self.m_Root.feature_index = feature  # Establece el índice de la característica\n",
        "          self.m_Root.threshold = threshold  # Establece el umbral de división\n",
        "          X_left = X[X[:, feature] <= threshold]\n",
        "          Y_left = Y[X[:, feature] <= threshold]\n",
        "          index_left = [index[i] for i in range(len(index)) if X[i, feature] <= threshold]\n",
        "\n",
        "          X_right = X[X[:, feature] > threshold]\n",
        "          Y_right = Y[X[:, feature] > threshold]\n",
        "          index_right = [index[i] for i in range(len(index)) if X[i, feature] > threshold]\n",
        "\n",
        "          # Crea un nodo no terminal y asigna a self.m_Root\n",
        "          self.m_Root.izquierda = DT(X_left, Y_left, index_left).m_Root\n",
        "          self.m_Root.derecha = DT(X_right, Y_right, index_right).m_Root\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for sample in X:\n",
        "            label = self.predict_sample(sample, self.m_Root)\n",
        "            predictions.append(label)\n",
        "        return predictions\n",
        "\n",
        "    def predict_sample(self, sample, node):\n",
        "        if node.label is not None:\n",
        "            return node.label\n",
        "\n",
        "        feature_index, threshold = node.feature_index, node.threshold\n",
        "        if threshold is not None:\n",
        "            if sample[feature_index] <= threshold:\n",
        "                return self.predict_sample(sample, node.izquierda)\n",
        "            else:\n",
        "                return self.predict_sample(sample, node.derecha)\n",
        "        else:\n",
        "\n",
        "            return node.label\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.drop('species', axis=1), iris['species'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear una instancia de DT con los datos de entrenamiento\n",
        "dt = DT(X_train.values, y_train.values, index=0)  # Cambia index a un valor único, en este caso, 0\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = dt.predict(X_test.values)\n",
        "\n",
        "# Calcula la matriz de confusión\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Imprime la matriz de confusión\n",
        "print(cm)\n",
        "\n",
        "accuracy = accuracy_score(y_test.values, y_pred)\n",
        "\n",
        "print(f\"Exactitud: {accuracy}\")"
      ]
    }
  ]
}