{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # Práctica  5.\n",
        " ----\n",
        "  \n",
        "  Universidad : UTEC \\\\\n",
        "  Curso       : Inteligencia Artificial \\\\\n",
        "  Profesor    : Cristian López Del Alamo \\\\\n",
        "  Tema        : Árboles de Decisión \\\\\n",
        "  \n",
        "\n",
        " ----\n",
        "\n",
        " Nombres y  Apellidos de Integrantes: (No olvide poner el % de participacion)\n",
        " - Integrante 1: Juan Sebastian Sara 100%\n",
        " - Integrante 2: Vasco Diaz Hurtado 100%\n",
        " - Integrante 3: Marcela Espinoza 100%\n",
        " - Integrante 4: Yamileth Rincón 100%\n",
        "\n",
        "*Una vez concluya la práctica debe subir el link de su colab a este  [Drive]\n",
        "(https://docs.google.com/spreadsheets/d/1XCxGVmf8g29C7RZSPOqxvZHjjRZg45LHWR8mkZOfS1o/edit?usp=drivesdk)*\n"
      ],
      "metadata": {
        "id": "9lQct68MlKo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este ejercicio, usted deberá contruir su propio árbol de desición.\n",
        "- Tenga en cuenta que se trata de un algoritmo recursivo.\n",
        "- El caso base se da cuanto todos los elementos de un nodo tiene las mismas etiquetas, es decir, es un nodo terminal. Luego, el label de ese nodo toma el valor de la etiqueta común.\n",
        "- En el caso que no sea un nodo terminal, el algoritmo debe buscar uno de los\n",
        "feactures por el cual dividirse y para esto use Ganacia de Información (Entropía o Gini).\n",
        "- Divida el dataset usando el feacture que genere una mayor ganancia de información en el padre o un menor GINI y llame recursivamente a la función create_DT.\n",
        "\n",
        "Usted usará la base de datos iris, con 4 características y 3 clases.\n",
        "Tome aleatoriamente 80% de los datos para crear el árbol y el resto para\n",
        "probar el accuracy de la predicción.\n",
        "Finalmente, muestra mediante una matriz de confusión el **accuracy** de su modelo.\n",
        "\n",
        "Trabaje en equipo:\n",
        "\n",
        "[Link de apoyo 1](https://towardsdatascience.com/the-simple-math-behind-3-decision-tree-splitting-criterions-85d4de2a75fe)\n",
        "\n",
        "[Link de apoyo 2](https://www.quantstart.com/articles/Beginners-Guide-to-Decision-Trees-for-Supervised-Machine-Learning/)\n",
        "\n"
      ],
      "metadata": {
        "id": "G19GO8Aflmlv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o74EDY9g86G9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654b887e-6682-4a67-9501-e28da1ce85e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sepal_length  sepal_width  petal_length  petal_width species\n",
            "0           5.1          3.5           1.4          0.2  setosa\n",
            "1           4.9          3.0           1.4          0.2  setosa\n",
            "2           4.7          3.2           1.3          0.2  setosa\n",
            "3           4.6          3.1           1.5          0.2  setosa\n",
            "4           5.0          3.6           1.4          0.2  setosa\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "iris = sns.load_dataset('iris')\n",
        "print(iris.head())\n",
        "\n",
        "\n",
        "class Nodo:\n",
        "    def __init__(self, X, Y, index=None):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.index = index\n",
        "        self.split_feature = None\n",
        "        self.split_value = None\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.label = None\n",
        "\n",
        "    def IsTerminal(self):\n",
        "        return len(np.unique(self.Y)) == 1\n",
        "\n",
        "    def Entropy(self, Y):\n",
        "        labels, counts = np.unique(Y, return_counts=True)\n",
        "        probs = counts / len(Y)\n",
        "        entropy = - np.sum(probs * np.log2(probs))\n",
        "        return entropy\n",
        "\n",
        "    def BestSplit(self):\n",
        "        best_gain = 0\n",
        "        best_feature = None\n",
        "        best_value = None\n",
        "\n",
        "        current_entropy = self.Entropy(self.Y)\n",
        "\n",
        "        for feature in self.index:\n",
        "            unique_values = np.unique(self.X[feature])\n",
        "            for value in unique_values:\n",
        "                left_Y = self.Y[self.X[feature] <= value]\n",
        "                right_Y = self.Y[self.X[feature] > value]\n",
        "                left_prob = len(left_Y) / len(self.Y)\n",
        "                right_prob = 1 - left_prob\n",
        "                new_entropy = left_prob * self.Entropy(left_Y) + right_prob * self.Entropy(right_Y)\n",
        "                gain = current_entropy - new_entropy\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_value = value\n",
        "\n",
        "        self.split_feature = best_feature\n",
        "        self.split_value = best_value\n",
        "\n",
        "        return best_feature, best_value\n",
        "\n",
        "\n",
        "class DT:\n",
        "    def __init__(self):\n",
        "        self.m_Root = None\n",
        "\n",
        "    def create_DT(self, X, Y, index=None):\n",
        "        if index is None:\n",
        "            index = X.columns.tolist()\n",
        "\n",
        "        node = Nodo(X, Y, index)\n",
        "\n",
        "        if node.IsTerminal():\n",
        "            node.label = Y.iloc[0]\n",
        "            return node\n",
        "\n",
        "        feature, value = node.BestSplit()\n",
        "\n",
        "        if feature is None:\n",
        "            node.label = Y.value_counts().idxmax()\n",
        "            return node\n",
        "\n",
        "        node.left = self.create_DT(X[X[feature] <= value], Y[X[feature] <= value], index)\n",
        "        node.right = self.create_DT(X[X[feature] > value], Y[X[feature] > value], index)\n",
        "\n",
        "        return node\n",
        "\n",
        "    def predict(self, node, x):\n",
        "        if node.label is not None:\n",
        "            return node.label\n",
        "\n",
        "        if x[node.split_feature] <= node.split_value:\n",
        "            return self.predict(node.left, x)\n",
        "        else:\n",
        "            return self.predict(node.right, x)\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        self.m_Root = self.create_DT(X, Y)\n",
        "\n",
        "\n",
        "X = iris.drop('species', axis=1)\n",
        "Y = iris['species']\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "tree = DT()\n",
        "tree.fit(X_train, Y_train)\n",
        "\n",
        "predictions = X_test.apply(lambda x: tree.predict(tree.m_Root, x), axis=1)\n",
        "print(confusion_matrix(Y_test, predictions))\n",
        "print('Accuracy:', accuracy_score(Y_test, predictions))\n"
      ]
    }
  ]
}